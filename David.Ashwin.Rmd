---
title: "cind110_Assignment_03"
author: "Ashwin David"
Due: "December 14, 2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

Use RStudio for this assignment. 
Edit the file `A3_F19_Q.Rmd` and insert your R code where wherever you see the string "#WRITE YOUR ANSWER HERE"

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

This assignment makes use of data that were adapted from:
https://www.ted.com/talks



#Install and load required packages (please install if required)
```{r}
#install.packages("tm")       
#install.packages("text2vec") 
#install.packages("NLP")
#install.packages("SnowballC")
#install.packages("slam")
#install.packages("textstem")
#install.packages("wordcloud")
#install.packages("Matrix")
#install.packages("Rcpp")
library(tm)
library(SnowballC)
library(NLP)
library(slam)
library(text2vec)
library(textstem)
library(wordcloud)
library(Matrix)
```


## Reading the Transcripts
```{r}
data <- read.csv(file = 'Sec-2-IR_Data.csv', header = F, sep = '|')
doc <- 0
for (i in c(2:100)) {doc[i] <- as.character(data$V1[i])}
doc.list <- as.list(doc[2:100])
N.docs <- length(doc.list)
names(doc.list) <- paste0("Doc", c(1:N.docs))
Query <- as.character(data$V1[1])
```

## Preparing the Corpus
```{r}
my.docs <- VectorSource(c(doc.list, Query))
my.docs$Names <- c(names(doc.list), "Query")
my.corpus <- Corpus(my.docs)
#my.corpus
```


## Cleaning and Preprocessing the text (Cleansing Techniques)
```{r}
#Write your answer here fro Question 1
#Hint: use getTransformations() function in tm Package
#https://cran.r-project.org/web/packages/tm/tm.pdf

#convert numbers to words
for (i in length(my.corpus)){
  my.corpus[[i]]$content <- as.character(textclean::replace_number(my.corpus[[i]]$content))
}
#updated_corpus1 <- my.corpus[[i]]$content

##utilizing a thesaurus
for(i in length(my.corpus)){
  my.corpus[[i]]$content <- textstem::lemmatize_strings(my.corpus[[i]]$content,dictionary = lexicon::hash_lemmas)
}

#stemming
my.corpus <- tm::tm_map(my.corpus, stemDocument)
my.corpus <- tm_map(my.corpus, content_transformer(tolower))

```


```{r}
#removing words of abundance using removeWords function
#my.corpus <- tm_map(my.corpus, removeWords, c("is", "the", "a", "are", "so", "what", "an"))
my.corpus <- tm_map(my.corpus, removeWords, stopwords("english"))
tm_map(my.corpus, removeWords, stopwords("smart"))
                   
#remove extra dashes and other punctuation marks that will affect the processing
my.corpus <- tm_map(my.corpus, removePunctuation, ucp = TRUE, preserve_intra_word_contractions = FALSE, preserver_intra_word_dashes = FALSE)

#strip off the white space
my.corpus <- tm_map(my.corpus, stripWhitespace)
```



##Creating a uni-gram Term Document Matrix (TDM)
```{r}
#write your answer here for Question 2
#Hint: use TermDocumentMatrix()
#creating a unigram tdm
tdmUnigram <- tm::TermDocumentMatrix(my.corpus) 
tm::inspect(tdmUnigram[1:10,1:10])
```


```{r}
#use remove Sparse to compute all terms after normalizing and refining the data
tdmUnigram <- tm::removeSparseTerms(tdmUnigram,0.67) 
tm::inspect(tdmUnigram[1:10,1:10])
```

## Converting the generated TDM into a matrix and displaying the first 6 rows and the dimensions of the matrix
```{r}
#write your answer here for Question 3
#Hint: use dim to find the dimension
tdmUnigramMat <- as.matrix(tdmUnigram)
dim(tdmUnigramMat)
head(tdmUnigramMat)

```

## Generate a wordcloud of the most occured 100 words across all transcripts
```{r}
#Write your answer here for Question 4
#Hint: use wordcloud
#set.seed(1)

words = sort(rowSums(tdmUnigramMat), decreasing = TRUE)
df = data.frame(word = names(words), freq = words)
head(df)
```


```{r}
set.seed(3)
wordcloud(words = df$word, freq = df$freq, min.freq = 1, max.words = 100, colors = brewer.pal(8, "Dark2"), random.order = FALSE, rot.per = 0.35, random.color = TRUE)

```



